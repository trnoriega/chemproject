{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEMA website information extraction\n",
    "\n",
    "### The library pages\n",
    "\n",
    "The FEMA website contains a series of [library pages](https://www.femaflavor.org/flavor/library?page=) that list all of the FEMA chemicals.\n",
    "\n",
    "The functions below extract all of the links available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "\n",
    "def link_to_soup(link, strainer=None):\n",
    "    '''\n",
    "    Makes a beautiful soup object from link. Disguises itself\n",
    "    as a browser so its not confused for a bot\n",
    "\n",
    "    strainer: limits the html to be parsed\n",
    "\n",
    "    returns:\n",
    "    -Soup object if one can be made\n",
    "    -None otherwise\n",
    "    '''\n",
    "    try:\n",
    "        req = Request(link, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        page = urlopen(req).read()\n",
    "        soup = BeautifulSoup(page, 'lxml', parse_only=strainer)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fema_link_finder():\n",
    "    \"\"\"\n",
    "    Creates a list of dictionaries with compound names, links and FEMA numbers\n",
    "    based on the FEMA website library pages\n",
    "    \"\"\"\n",
    "    fema_library_link = 'http://www.femaflavor.org/flavor/library?page='\n",
    "    fema_base_link = 'http://www.femaflavor.org'\n",
    "    strainer = SoupStrainer('tbody')\n",
    "    ret_list = []\n",
    "    for i in range(28):\n",
    "        new_link = fema_library_link + str(i)\n",
    "        soup = link_to_soup(new_link, strainer=strainer)\n",
    "        rows = soup.findAll('tr')\n",
    "        \n",
    "        for row in rows:\n",
    "            columns = row.find_all('td')\n",
    "            for col in columns:\n",
    "                if col.string:\n",
    "                    num = int(col.string)\n",
    "                    #print(num)\n",
    "                elif col.a:\n",
    "                    name = str(col.a.string).lower()\n",
    "                    full_link = fema_base_link + col.a.get('href')\n",
    "                    #print(name, full_link)\n",
    "            dicto = {'name': name, 'link': full_link, 'fema': num}\n",
    "            ret_list.append(dicto)\n",
    "            print('.', end='')\n",
    "    \n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fema_links = fema_link_finder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fema': 2001,\n",
       " 'link': 'http://www.femaflavor.org/acacia-gum-acacia-senegal-l-willd-2',\n",
       " 'name': 'acacia gum (acacia senegal (l.) willd.)'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fema_links[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intermediate data dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os.path as path\n",
    "\n",
    "DATA_PATH = path.join(path.expanduser('~'),\n",
    "                     'Dropbox',\n",
    "                     'bymt',\n",
    "                     'data_dumps',\n",
    "                     'chem_project',\n",
    "                     'fema_extraction')\n",
    "\n",
    "fema_links_path = path.join(DATA_PATH, 'fema_links.pkl')\n",
    "\n",
    "# with open(fema_links_path, 'wb') as f:\n",
    "#     pickle.dump(fema_links, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(fema_links_path, 'rb') as f:\n",
    "    fema_links = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each chemical then has its own page (for example, [acetic acid](https://www.femaflavor.org/acetic-acid-2)) from which I will extract:\n",
    "- Flavor descriptors\n",
    "- Chemical Abstracts Service (CAS) registry number\n",
    "- JECFA number\n",
    "- US Government's Code of Ferderal Regulations (CFR) citation\n",
    "\n",
    "The folowing functions take the data from `fema_links` to get the data from each individual chemical page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from chemspipy import ChemSpider\n",
    "cs = ChemSpider('0201ba66-585d-4135-9e6b-d28ba4724fcf')\n",
    "from rdkit import Chem\n",
    "\n",
    "def fema_link_info(dicto):\n",
    "    \"\"\"\n",
    "    Create a copy of dicto with extracted information from the FEMA website added\n",
    "    \"\"\"\n",
    "    link_dict = dicto.copy()\n",
    "    soup = link_to_soup(link_dict['link'])\n",
    "    if soup: \n",
    "        # Get the page title fema number and confirm it matches the number from link_dict\n",
    "        page_titles = soup.find_all('h2', class_='pageTitle')\n",
    "        for res in page_titles:\n",
    "            if len(res.text) > 0:\n",
    "                title = res.text.split('|')\n",
    "                title = [word.strip() for word in title]\n",
    "                title_num = int(title[-1]) #compound number\n",
    "        if title_num != link_dict['fema']:\n",
    "            print('FEMA # from link does not match page title', end=' ')\n",
    "            return None\n",
    "        \n",
    "        # Get the page headings and extract their information\n",
    "        page_headings = soup.find_all('div', class_='field field-type-header')\n",
    "        for item in page_headings:\n",
    "            try:\n",
    "                label = item.find('h3', class_='field-label').stripped_strings\n",
    "                label = list(label)[0]\n",
    "                content = item.find('div', class_='field-item').stripped_strings\n",
    "                content = list(content)[0]\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            if label == 'FLAVOR PROFILE':\n",
    "                link_dict['descriptors'] = content\n",
    "                #lowercase, remove non-word characters (function1), and reduce words\n",
    "                # to their stem (function2)\n",
    "                content.lower()\n",
    "                pattern = re.compile('[\\W_]+')\n",
    "                pattern.sub(' ', content)\n",
    "                stemmer = nltk.stem.SnowballStemmer('english')\n",
    "                stems = [stemmer.stem(word) for word in content.split(' ')]\n",
    "                stems = ' '.join(stems)\n",
    "                link_dict['stems'] = stems\n",
    "            elif label == 'CAS':\n",
    "                link_dict['cas'] = content\n",
    "            elif label == 'JECFA NUMBER':\n",
    "                link_dict['jecfa'] = content\n",
    "            elif label == 'CFR':\n",
    "                link_dict['cfr'] = content\n",
    "        \n",
    "        return link_dict\n",
    "    \n",
    "    else:\n",
    "        print('No soup could be make from the link found')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printed_fema_extraction(dicto_list):\n",
    "    \"\"\"\n",
    "    Applies fema_link_info function to a list of individual chemical dictionaries.\n",
    "    Displays a readout so that progress is known.\n",
    "    \"\"\"\n",
    "    \n",
    "    out = []\n",
    "    total = len(dicto_list)\n",
    "    count = 0\n",
    "    last_displayed = 0\n",
    "    \n",
    "    for dicto in dicto_list:\n",
    "        out.append(fema_link_info(dicto))\n",
    "        \n",
    "        # This noise is all about a nice display with percentage completed\n",
    "        count += 1\n",
    "        val = round((count / total) * 100)\n",
    "        if (val % 5 == 0 and\n",
    "            val != last_displayed):\n",
    "            print('{:2.0f}%' .format(val), end = '.')\n",
    "        else:\n",
    "            print('.', end='')\n",
    "        last_displayed = val\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fema_chunker(chunkable, file_name='extracted_fema.pkl', splits=10, chunk_list=None):\n",
    "    \"\"\"\n",
    "    Splits the extraction of individual chemical information into separate chunks.\n",
    "    As each chunk is completed it is saved into an updated pickle file. \n",
    "    chunk_list can specify particular chunks to be processed. \n",
    "    \"\"\"\n",
    "    total = len(chunkable)\n",
    "    \n",
    "    # determine chunk size\n",
    "    chunk_size, mod = total//(splits), total%splits\n",
    "    if mod != 0:\n",
    "        chunk_size = total//(splits-1)\n",
    "        mod = total%chunk_size\n",
    "        if (mod == 0 or\n",
    "           mod < chunk_size/2): # This makes sure that the remainder is not too large\n",
    "            chunk_size -= round(chunk_size/(2*splits))\n",
    "    print('Chunk size: {}' .format(chunk_size))\n",
    "    \n",
    "    # Generate a list with the chunk indices so that if a specific chunk number is specified\n",
    "    # it can be found and generated consistently\n",
    "    start = 0\n",
    "    end = chunk_size\n",
    "    start_end_list = []\n",
    "    while end != total:\n",
    "        start_end_list.append((start, end))\n",
    "        start += chunk_size\n",
    "        end += chunk_size\n",
    "        if end > total:\n",
    "            end = total\n",
    "    start_end_list.append((start,end))\n",
    "    print('Number of chunks: {}' .format(len(start_end_list)))\n",
    "    \n",
    "    # This part dues the actual extraction from the FEMA website\n",
    "    extracted_fema_path = path.join(DATA_PATH, file_name)\n",
    "    extracted_fema = []\n",
    "    \n",
    "    if not chunk_list:\n",
    "        iterable = enumerate(start_end_list)\n",
    "    else:\n",
    "        sub_is = [start_end_list[i] for i in chunk_list]\n",
    "        iterable = enumerate(sub_is)\n",
    "    \n",
    "    for i, tup in iterable:\n",
    "        print ('\\nChunk number {}, start: {}, end: {}' .format(i, tup[0], tup[1]))\n",
    "        chunk = chunkable[tup[0]:tup[1]]\n",
    "        extracted_fema += printed_fema_extraction(chunk)\n",
    "        \n",
    "        # Save after every chunk\n",
    "        with open(extracted_fema_path, 'wb') as f:\n",
    "            pickle.dump(extracted_fema, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "       \n",
    "    return extracted_fema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk size: 294\n",
      "Number of chunks: 10\n",
      "\n",
      "Chunk number 0, start: 0, end: 294\n",
      "............. 5%..............10%...............15%...............20%...............25%..............30%...............35%...............40%..............45%...............50%...............55%..............60%...............65%...............70%...............75%..............80%...............85%...............90%..............95%...............100%..\n",
      "Chunk number 1, start: 294, end: 588\n",
      "............. 5%..............10%...............15%...............20%...............25%..............30%...............35%...............40%..............45%...............50%...............55%..............60%...............65%...............70%...............75%..............80%...............85%...............90%..............95%...............100%..\n",
      "Chunk number 2, start: 588, end: 882\n",
      "............. 5%..............10%...............15%...............20%...............25%..............30%...............35%...............40%..............45%...............50%...............55%..............60%...............65%...............70%...............75%..............80%...............85%...............90%..............95%...............100%..\n",
      "Chunk number 3, start: 882, end: 1176\n",
      "............. 5%..............10%...............15%...............20%...............25%..............30%...............35%...............40%..............45%...............50%...............55%..............60%....No soup could be make from the link found\n",
      "...........65%...............70%...............75%..............80%...............85%...............90%..............95%...............100%..\n",
      "Chunk number 4, start: 1176, end: 1470\n",
      "............. 5%..............10%...............15%...............20%...............25%..............30%...............35%...............40%..............45%...............50%...............55%..............60%...............65%...............70%...............75%..............80%...............85%...............90%..............95%...............100%..\n",
      "Chunk number 5, start: 1470, end: 1764\n",
      "............. 5%..............10%...............15%...............20%...............25%..............30%...............35%...............40%..............45%...............50%...............55%..............60%...............65%...............70%...............75%..............80%...............85%...............90%..............95%...............100%..\n",
      "Chunk number 6, start: 1764, end: 2058\n",
      "............. 5%..............10%...............15%...............20%...............25%..............30%...............35%...............40%..............45%...............50%...............55%..............60%...............65%...............70%...............75%..............80%...............85%...............90%..............95%...............100%..\n",
      "Chunk number 7, start: 2058, end: 2352\n",
      "............. 5%..............10%...............15%...............20%...............25%..............30%...............35%...............40%..............45%...............50%...............55%..............60%...............65%...............70%...............75%..............80%...............85%...............90%..............95%...............100%..\n",
      "Chunk number 8, start: 2352, end: 2646\n",
      "............. 5%..............10%...............15%...............20%...............25%..............30%...............35%...............40%..............45%...............50%...............55%..............60%...............65%...............70%...............75%..............80%...............85%...............90%..............95%...............100%..\n",
      "Chunk number 9, start: 2646, end: 2795\n",
      "...... 5%........10%.......15%........20%.......25%.......30%........35%.......40%........45%.......50%........55%.......60%........65%.......70%........75%.......80%.......85%........90%.......95%........100%."
     ]
    }
   ],
   "source": [
    "extracted_fema = fema_chunker(fema_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There was an error in chunk # 3 so I will retry it and add it back to extracted_fema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk size: 294\n",
      "Number of chunks: 10\n",
      "\n",
      "Chunk number 0, start: 882, end: 1176\n",
      "............. 5%..............10%...............15%...............20%...............25%..............30%...............35%...............40%..............45%...............50%...............55%..............60%...............65%...............70%...............75%..............80%...............85%...............90%..............95%...............100%.."
     ]
    }
   ],
   "source": [
    "chunk_3 = fema_chunker(fema_links, file_name='chunk3.pkl', chunk_list=[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(extracted_fema.index(None))\n",
    "extracted_fema[882:1176] = chunk_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "None is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-b094cf565b18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextracted_fema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: None is not in list"
     ]
    }
   ],
   "source": [
    "print(extracted_fema.index(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length extracted dictionary: 2795 compared to length of links: 2795\n"
     ]
    }
   ],
   "source": [
    "print('Length extracted dictionary: {} compared to length of links: {}' \n",
    "      .format(len(extracted_fema), len(fema_links)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data dump of the final list of dictionaries with the FEMA website information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extracted_fema_path = path.join(DATA_PATH, 'extracted_fema.pkl')\n",
    "with open(extracted_fema_path, 'wb') as f:\n",
    "    pickle.dump(extracted_fema, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
