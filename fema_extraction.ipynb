{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEMA website information extraction\n",
    "\n",
    "### The library pages\n",
    "\n",
    "The FEMA website contains a series of [library pages](https://www.femaflavor.org/flavor/library?page=) that list all of the FEMA chemicals.\n",
    "\n",
    "The functions below extract all of the links available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "\n",
    "def link_to_soup(link, strainer=None):\n",
    "    '''\n",
    "    support function makes a beautiful soup object from link. Disguises itself\n",
    "    as a browser so its not confused for a bot\n",
    "\n",
    "    input:\n",
    "    link: to use as the source for the Beautiful soup object\n",
    "    strainer: can limit the output soup object to a specific type of content\n",
    "\n",
    "    returns:\n",
    "    -Soup object if one can be made\n",
    "    -None otherwise\n",
    "    '''\n",
    "    try:\n",
    "        req = Request(link, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        page = urlopen(req).read()\n",
    "        soup = BeautifulSoup(page, 'lxml', parse_only=strainer)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fema_link_finder():\n",
    "    \"\"\"\n",
    "    Creates a list of dictionaries with compound names, links and FEMA numbers\n",
    "    based on the FEMA website library pages\n",
    "    \"\"\"\n",
    "    fema_library_link = 'http://www.femaflavor.org/flavor/library?page='\n",
    "    fema_base_link = 'http://www.femaflavor.org'\n",
    "    strainer = SoupStrainer('tbody')\n",
    "    ret_list = []\n",
    "    for i in range(28):\n",
    "        new_link = fema_library_link + str(i)\n",
    "        soup = link_to_soup(new_link, strainer=strainer)\n",
    "        rows = soup.findAll('tr')\n",
    "        \n",
    "        for row in rows:\n",
    "            columns = row.find_all('td')\n",
    "            for col in columns:\n",
    "                if col.string:\n",
    "                    num = int(col.string)\n",
    "                    #print(num)\n",
    "                elif col.a:\n",
    "                    name = str(col.a.string).lower()\n",
    "                    full_link = fema_base_link + col.a.get('href')\n",
    "                    #print(name, full_link)\n",
    "            dicto = {'name': name, 'link': full_link, 'fema': num}\n",
    "            ret_list.append(dicto)\n",
    "            print('.', end='')\n",
    "    \n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fema_links = fema_link_finder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fema_links[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intermediate data dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os.path as path\n",
    "\n",
    "DATA_PATH = path.join(path.expanduser('~'),\n",
    "                     'Dropbox',\n",
    "                     'bymt',\n",
    "                     'data_dumps',\n",
    "                     'chem_project',\n",
    "                     'fema_extraction')\n",
    "\n",
    "fema_links_path = path.join(DATA_PATH, 'fema_links.pkl')\n",
    "\n",
    "# with open(fema_links_path, 'wb') as f:\n",
    "#     pickle.dump(fema_links, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(fema_links_path, 'rb') as f:\n",
    "    fema_links = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each chemical then has its own page (for example, [acetic acid](https://www.femaflavor.org/acetic-acid-2)) from which I will extract:\n",
    "- Flavor descriptors\n",
    "- Chemical Abstracts Service (CAS) registry number\n",
    "- JECFA number\n",
    "- US Government's Code of Ferderal Regulations (CFR) citation\n",
    "\n",
    "The folowing functions take the data from `fema_links` to get the data from each individual chemical page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from chemspipy import ChemSpider\n",
    "cs = ChemSpider('0201ba66-585d-4135-9e6b-d28ba4724fcf')\n",
    "from rdkit import Chem\n",
    "\n",
    "def link_info(dicto):\n",
    "    \"\"\"\n",
    "    Extract and add descriptors, stems, CAS, JECFA, and CFR numbers to the link_dict,\n",
    "    based on the link provided within link_dict\n",
    "    \"\"\"\n",
    "    link_dict = dicto.copy()\n",
    "    soup = link_to_soup(link_dict['link'])\n",
    "    if soup: \n",
    "        # Get the page title fema number and confirm it matches the number from link_dict\n",
    "        page_titles = soup.find_all('h2', class_='pageTitle')\n",
    "        for res in page_titles:\n",
    "            if len(res.text) > 0:\n",
    "                title = res.text.split('|')\n",
    "                title = [word.strip() for word in title]\n",
    "                title_num = int(title[-1]) #compound number\n",
    "        if title_num != link_dict['fema']:\n",
    "            print('FEMA # from link does not match page title', end=' ')\n",
    "            return None\n",
    "        \n",
    "        # Get the page headings and extract their information\n",
    "        page_headings = soup.find_all('div', class_='field field-type-header')\n",
    "        for item in page_headings:\n",
    "            try:\n",
    "                label = item.find('h3', class_='field-label').stripped_strings\n",
    "                label = list(label)[0]\n",
    "                content = item.find('div', class_='field-item').stripped_strings\n",
    "                content = list(content)[0]\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            if label == 'FLAVOR PROFILE':\n",
    "                link_dict['descriptors'] = content\n",
    "                #lowercase, remove non-word characters (function1), and reduce words\n",
    "                # to their stem (function2)\n",
    "                content.lower()\n",
    "                pattern = re.compile('[\\W_]+')\n",
    "                pattern.sub(' ', content)\n",
    "                stemmer = nltk.stem.SnowballStemmer('english')\n",
    "                stems = [stemmer.stem(word) for word in content.split(' ')]\n",
    "                stems = ' '.join(stems)\n",
    "                link_dict['stems'] = stems\n",
    "            elif label == 'CAS':\n",
    "                link_dict['cas'] = content\n",
    "            elif label == 'JECFA NUMBER':\n",
    "                link_dict['jecfa'] = content\n",
    "            elif label == 'CFR':\n",
    "                link_dict['cfr'] = content\n",
    "        \n",
    "        return link_dict\n",
    "    \n",
    "    else:\n",
    "        print('No soup could be make from the link found')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printed_fema_extraction(dicto_list):\n",
    "    \"\"\"\n",
    "    Create a list of dictionaries with all of the extractable FEMA info.\n",
    "    Displays a readout so that progress is known\n",
    "    \"\"\"\n",
    "    \n",
    "    out = []\n",
    "    total = len(dicto_list)\n",
    "    count = 0\n",
    "    last_displayed = 0\n",
    "    \n",
    "    for dicto in dicto_list:\n",
    "        out.append(link_info(dicto))\n",
    "        \n",
    "        # This noise is all about a nice display with percentage completed\n",
    "        count += 1\n",
    "        val = round((count / total) * 100)\n",
    "        if (val % 5 == 0 and\n",
    "            val != last_displayed):\n",
    "            print('{:2.0f}%' .format(val), end = '.')\n",
    "        else:\n",
    "            print('.', end='')\n",
    "        last_displayed = val\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-12-ddfbac50a4ac>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-ddfbac50a4ac>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    for i = range(1,11):\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "for i = range(1,11):\n",
    "    print 280*i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............ 5%..............10%..............15%..............20%..............25%..............30%..............35%..............40%..............45%..............50%..............55%..............60%..............65%..............70%..............75%..............80%..............85%..............90%..............95%..............100%.."
     ]
    }
   ],
   "source": [
    "extracted_fema_1 = printed_fema_extraction(fema_links[:280])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............ 5%..............10%..............15%..............20%..............25%..............30%..............35%..............40%..............45%..............50%..............55%..............60%..............65%..............70%..............75%..............80%..............85%..............90%..............95%..............100%.."
     ]
    }
   ],
   "source": [
    "extracted_fema_2 = printed_fema_extraction(fema_links[280:560])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............ 5%..............10%..............15%..............20%..............25%..............30%..............35%..............40%..............45%..............50%..............55%..............60%..............65%..............70%..............75%..............80%..............85%..............90%..............95%..............100%.."
     ]
    }
   ],
   "source": [
    "extracted_fema_3 = printed_fema_extraction(fema_links[560:840])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_fema = extracted_fema_1 + extracted_fema_2 + extracted_fema_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extracted_fema_path = path.join(DATA_PATH, 'extracted_fema.pkl')\n",
    "with open(extracted_fema_path, 'wb') as f:\n",
    "    pickle.dump(extracted_fema, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extracted_fema_4 = printed_fema_extraction(fema_links[840:1120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extracted_fema_5 = printed_fema_extraction(fema_links[1120:1400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extracted_fema_6 = printed_fema_extraction(fema_links[1400:1680])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extracted_fema_7 = printed_fema_extraction(fema_links[1680:1960])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "import re\n",
    "\n",
    "from chemspipy import ChemSpider\n",
    "cs = ChemSpider('0201ba66-585d-4135-9e6b-d28ba4724fcf')\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from inspect import getmembers, isfunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_and_filter(number,\n",
    "                      search_prefix='http://www.femaflavor.org/search/apachesolr_search/',\n",
    "                      substring='/flavor/library/'):\n",
    "    '''\n",
    "    support function for dictionary_maker\n",
    "    searches the Fema website for the number given and\n",
    "    returns a list of links that contain the substring.\n",
    "    Returns None otherwise\n",
    "\n",
    "    Inputs:\n",
    "    -number: Fema number to search for\n",
    "    -search_prefix: web address prefix to search in\n",
    "    -substring: to filter results\n",
    "\n",
    "    Returns:\n",
    "    -page_headings\n",
    "    -name\n",
    "    -link\n",
    "\n",
    "    or\n",
    "    -None if none are found\n",
    "    '''\n",
    "\n",
    "\n",
    "    search_link = search_prefix + str(number)\n",
    "    soup = link_to_soup(search_link)\n",
    "    if soup:\n",
    "        search_block = soup.find_all('dl', class_='search-results apachesolr_search-results')\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    #See if there are any results and extract only the links to flavor compounds\n",
    "    try:\n",
    "        titles = search_block[0].find_all('dt', class_='title')\n",
    "        #extract all search result links\n",
    "        links = [title.find('a').get('href') for title in titles]\n",
    "        #select only links with flavor compund substring\n",
    "        links_checked = [link for link in links if substring in link]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    if len(links_checked) >= 1:\n",
    "        for link in links_checked:\n",
    "            print(link)\n",
    "            soup = link_to_soup(link)\n",
    "            if soup:\n",
    "                page_title = soup.find('h2', class_='pageTitle')\n",
    "                page_headings = soup.find_all('div', class_='field field-type-header')\n",
    "                title = page_title.text.split('|')\n",
    "                title = [word.strip() for word in title]\n",
    "                name = title[0] #compound name\n",
    "                title_num = title[-1] #compound number\n",
    "                if title_num == str(number):\n",
    "                    return page_headings, name, link\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chem_search(dict_entry, priotity_list):\n",
    "    '''\n",
    "    returns a rdkit molecule after searching the chemspider database based on the items\n",
    "    in the priority list.\n",
    "    '''\n",
    "\n",
    "    for tup in priotity_list:\n",
    "        try:\n",
    "            tup_string = dict_entry.get(tup[1])\n",
    "        except AttributeError:\n",
    "            continue\n",
    "\n",
    "        if tup_string:\n",
    "            search_string = tup[0] + tup_string\n",
    "            #print('searching for: {}' .format(search_string))\n",
    "            results = cs.search(search_string)\n",
    "            #print('stopped searching')\n",
    "            if same_chemical(results):\n",
    "                #print(tup)\n",
    "                return same_chemical(results)\n",
    "            else:\n",
    "                continue\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dictionary_maker(num_iterator):\n",
    "    '''\n",
    "    returns a dictionary of chemicals found in the femaflavor.org website with FEMA numbers in\n",
    "    the given num_iterator\n",
    "\n",
    "    inputs:\n",
    "    -num_iterator: an iterable object with the fema numbers to be searched\n",
    "\n",
    "    returns:\n",
    "    dictionary with fema number as primary key and the following subkeys:\n",
    "    'link','name', 'descriptors', 'CAS', 'JECFA', 'CFR'\n",
    "    '''\n",
    "\n",
    "    dictionary = {}\n",
    "    count = 0\n",
    "    priority_list = [('fema ', 'FEMA'), ('jecfa ', 'JECFA'), ('', 'CAS'), ('', 'name')]\n",
    "\n",
    "    for number in num_iterator:\n",
    "        #searchNameLink is (pageHeadings, name, link) if there is a FEMA website for number.\n",
    "        # None otherwise\n",
    "        page_name_link = search_and_filter(number)\n",
    "\n",
    "        if page_name_link:\n",
    "            #Add all information from FEMA webpage to dictionary[number][subentries]\n",
    "            dictionary[number] = {}\n",
    "            dictionary[number]['link'] = page_name_link[2]\n",
    "            dictionary[number]['name'] = page_name_link[1]\n",
    "            dictionary[number]['FEMA'] = str(number)\n",
    "            for item in page_name_link[0]:\n",
    "                try:\n",
    "                    label = item.find('h3', class_='field-label').stripped_strings\n",
    "                    label = list(label)[0]\n",
    "                    content = item.find('div', class_='field-item').stripped_strings\n",
    "                    content = list(content)[0]\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                if label == 'FLAVOR PROFILE':\n",
    "                    dictionary[number]['descriptors'] = content\n",
    "                    #lowercase, remove non-word characters (function1), and reduce words\n",
    "                    # to their stem (function2)\n",
    "                    content.lower()\n",
    "                    pattern = re.compile('[\\W_]+')\n",
    "                    pattern.sub(' ', content)\n",
    "                    stemmer = nltk.stem.SnowballStemmer('english')\n",
    "                    stems = [stemmer.stem(word) for word in content.split(' ')]\n",
    "                    stems = ' '.join(stems)\n",
    "                    text = nltk.word_tokenize(stems)\n",
    "                    tokens = nltk.pos_tag(text)\n",
    "                    selected = [token[0] for token in tokens if token[1] in ['NN', 'JJ']]\n",
    "                    dictionary[number]['tokens'] = selected\n",
    "                elif label == 'CAS':\n",
    "                    dictionary[number]['CAS'] = content\n",
    "                elif label == 'JECFA NUMBER':\n",
    "                    dictionary[number]['JECFA'] = content\n",
    "                elif label == 'CFR':\n",
    "                    dictionary[number]['CFR'] = content\n",
    "\n",
    "            #Add rdkit molecule to dictionary[number]['rdkit Mol']\n",
    "            test = chem_search(dictionary[number], priority_list)\n",
    "            if test:\n",
    "                dictionary[number]['rdkit Mol'] = test\n",
    "            else:\n",
    "                print(' {}nMol' .format(number), end='')\n",
    "\n",
    "        else:\n",
    "            print(' {}nLink' .format(number), end='')\n",
    "\n",
    "        count += 1\n",
    "        if count%10 == 0:\n",
    "            print(' {:.2f}%' .format(count/len(num_iterator)*100), end='')\n",
    "        else:\n",
    "            print('.', end='')\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def same_chemical(results):\n",
    "    '''\n",
    "    returns an rdkit chemical object if a the chemicals in a chemspipy result list have:\n",
    "    -the same molecular weight, and\n",
    "    -the same smiles representation\n",
    "    returns None otherwise\n",
    "    '''\n",
    "    if results.count == 0:\n",
    "        return None\n",
    "\n",
    "    smiles = []\n",
    "    mws = []\n",
    "\n",
    "    if results.count >= 1:\n",
    "        for chemical in results:\n",
    "            try:\n",
    "                smiles_base = chemical.smiles\n",
    "                chem_base = Chem.MolFromSmiles(smiles_base)\n",
    "\n",
    "                smiles_temp = Chem.MolToSmiles(chem_base)\n",
    "                smiles.append(smiles_temp)\n",
    "\n",
    "                mw_temp = Chem.Descriptors.MolWt(chem_base)\n",
    "                mws.append(mw_temp)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        if (len(set(smiles)) == 1 and\n",
    "                len(set(mws)) == 1):\n",
    "            return Chem.MolFromSmiles(Chem.MolToSmiles(chem_base))\n",
    "\n",
    "    else:\n",
    "        return None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
