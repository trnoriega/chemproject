{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support functions for dictMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "from chemspipy import ChemSpider\n",
    "cs = ChemSpider('0201ba66-585d-4135-9e6b-d28ba4724fcf')\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from inspect import getmembers, isfunction\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "def linkToSoup(link):\n",
    "    '''\n",
    "    support function for dictMaker and Search and Filter.\n",
    "    makes a beautiful soup object from link. Disguises itself as a browser so its not confused for a bot\n",
    "\n",
    "    input:\n",
    "    link: to use as the source for the Beautiful soup object\n",
    "\n",
    "    returns:\n",
    "    -Soup object if one can be made\n",
    "    -None otherwise\n",
    "    '''\n",
    "    try:\n",
    "        req = Request(link, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        page = urlopen(req).read()\n",
    "        soup = BeautifulSoup(page, 'lxml')\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    return soup\n",
    "\n",
    "def searchAndFilter(number,\n",
    "                    searchPrefix='http://www.femaflavor.org/search/apachesolr_search/',\n",
    "                    subString='/flavor/library/'):\n",
    "    '''\n",
    "    support function for dictMaker\n",
    "    searches the Fema website for the number given and returns a list of links that contain the subString. \n",
    "    Returns None otherwise\n",
    "    \n",
    "    Inputs:\n",
    "    -number: Fema number to search for\n",
    "    -searchPrefix: web address prefix to search in\n",
    "    -subString: to filter results\n",
    "    \n",
    "    Returns:\n",
    "    -pageHeadings\n",
    "    -name\n",
    "    -link\n",
    "    \n",
    "    or \n",
    "    -None if none are found\n",
    "    '''\n",
    "    \n",
    "    searchLink = searchPrefix + str(number)\n",
    "    soup = linkToSoup(searchLink)\n",
    "    if soup:\n",
    "        searchBlock = soup.find_all('dl', class_='search-results apachesolr_search-results')\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    #See if there are any results and extract only the links to flavor compounds\n",
    "    try:\n",
    "        titles = searchBlock[0].find_all('dt', class_='title')\n",
    "        links = [title.find('a').get('href') for title in titles] #extracts all search result links\n",
    "        linksChecked = [link for link in links if subString in link] #selects only links with flavor compund substring\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    if len(linksChecked) >= 1:\n",
    "        for link in linksChecked:\n",
    "            soup = linkToSoup(link)\n",
    "            if soup:\n",
    "                pageTitle = soup.find('h2', class_='pageTitle')\n",
    "                pageHeadings = soup.find_all('div', class_='field field-type-header')\n",
    "                title = pageTitle.text.split('|')\n",
    "                title = [word.strip() for word in title]\n",
    "                name = title[0] #compound name\n",
    "                titleNo = title[-1] #compound number\n",
    "                if titleNo == str(number):\n",
    "                    return pageHeadings, name, link\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def sameChemical(results):\n",
    "    '''\n",
    "    returns an rdkit chemical object if a the chemicals in a chemspipy result list have:\n",
    "    -the same molecular weight, and\n",
    "    -the same smiles representation\n",
    "    returns None otherwise\n",
    "    '''\n",
    "    if results.count == 0:\n",
    "        return None\n",
    "    \n",
    "    smiles = []\n",
    "    mws = []\n",
    "    \n",
    "    if results.count >= 1:\n",
    "        for chemical in results:\n",
    "            try:\n",
    "                smilesBase = chemical.smiles\n",
    "                chemBase = Chem.MolFromSmiles(smilesBase)\n",
    "\n",
    "                smilesTemp = Chem.MolToSmiles(chemBase)\n",
    "                smiles.append(smilesTemp)\n",
    "\n",
    "                mwTemp = Chem.Descriptors.MolWt(chemBase)\n",
    "                mws.append(mwTemp)\n",
    "            except:\n",
    "                continue\n",
    "        if (len(set(smiles)) == 1 and\n",
    "           len(set(mws)) == 1):\n",
    "            return Chem.MolFromSmiles(Chem.MolToSmiles(chemBase))\n",
    "    \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def chemSearch(femaDictEntry, priorityList):\n",
    "    '''\n",
    "    returns a rdkit molecule after searching the chemspider database based on the items\n",
    "    in the priority list.\n",
    "    '''\n",
    "    \n",
    "    for tup in priorityList:\n",
    "        \n",
    "        try:\n",
    "            t = femaDictEntry.get(tup[1])\n",
    "        except AttributeError:\n",
    "            continue\n",
    "        \n",
    "        if t:\n",
    "            searchString = tup[0] + t\n",
    "            #print('searching for: {}' .format(searchString))\n",
    "            results = cs.search(searchString)\n",
    "            #print('stopped searching')\n",
    "            if sameChemical(results):\n",
    "                #print(tup)\n",
    "                return sameChemical(results)\n",
    "            else:\n",
    "                continue\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Find individual FEMA website links for each chemical, which have the flavor information in them \n",
    "\n",
    "def dictMaker(numberIter):\n",
    "    '''\n",
    "    returns a dictionary of chemicals found in the femaflavor.org website with FEMA numbers in\n",
    "    the given numberIter\n",
    "    \n",
    "    inputs:\n",
    "    -numberIter: an iterable object with the fema numbers to be searched\n",
    "\n",
    "    returns:\n",
    "    dicty with fema number as primary key and the following subkeys:\n",
    "    'link','name', 'descriptors', 'CAS', 'JECFA', 'CFR'\n",
    "    '''\n",
    "\n",
    "    dicty = {}\n",
    "    count = 0\n",
    "    priorityList = [('fema ', 'FEMA'), ('jecfa ', 'JECFA'), ('', 'CAS'), ('', 'name')]\n",
    "\n",
    "    for number in numberIter:    \n",
    "        #searchNameLink is (pageHeadings, name, link) if there is a FEMA website for number. None otherwise\n",
    "        pageNameLink = searchAndFilter(number)\n",
    "        \n",
    "        \n",
    "        if pageNameLink:\n",
    "            \n",
    "            #Add all information from FEMA webpage to dicty[number][subentries]\n",
    "            dicty[number] = {}\n",
    "            dicty[number]['link'] = pageNameLink[2]\n",
    "            dicty[number]['name'] = pageNameLink[1]\n",
    "            dicty[number]['FEMA'] = str(number)\n",
    "            for item in pageNameLink[0]:\n",
    "                \n",
    "                try:\n",
    "                    label = item.find('h3', class_='field-label').stripped_strings\n",
    "                    label = list(label)[0]\n",
    "                    content = item.find('div', class_='field-item').stripped_strings\n",
    "                    content = list(content)[0]\n",
    "                except:\n",
    "                    continue\n",
    "                \n",
    "                if label == 'FLAVOR PROFILE':\n",
    "                    dicty[number]['descriptors'] = content\n",
    "                    #lowercase, remove non-word characters (function1), and reduce words to their stem (function2)\n",
    "                    content.lower()\n",
    "                    pattern = re.compile('[\\W_]+')\n",
    "                    pattern.sub(' ', content)\n",
    "                    stemmer = nltk.stem.SnowballStemmer('english')\n",
    "                    stems = [stemmer.stem(word) for word in content.split(' ')]\n",
    "                    stems = ' '.join(stems)\n",
    "                    text = nltk.word_tokenize(stems)\n",
    "                    tokens = nltk.pos_tag(text)\n",
    "                    selected = [token[0] for token in tokens if token[1] in ['NN', 'JJ']]\n",
    "                    dicty[number]['tokens'] = selected\n",
    "                elif label == 'CAS':\n",
    "                    dicty[number]['CAS']=content\n",
    "                elif label == 'JECFA NUMBER':\n",
    "                    dicty[number]['JECFA']=content\n",
    "                elif label == 'CFR':\n",
    "                    dicty[number]['CFR']=content\n",
    "            \n",
    "            #Add rdkit molecule to dicty[number]['rdkit Mol']\n",
    "            test = chemSearch(dicty[number], priorityList)\n",
    "            if test:\n",
    "                dicty[number]['rdkit Mol'] = test\n",
    "            else:\n",
    "                print(' {}nMol' .format(number), end='')\n",
    "\n",
    "        else:\n",
    "            print(' {}nLink' .format(number), end='')\n",
    "        \n",
    "        count += 1\n",
    "        if count%10 == 0:\n",
    "            print(' {:.2f}%' .format(count/len(numberIter)*100), end='')\n",
    "        else:\n",
    "            print('.', end='')\n",
    "    return dicty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. 4502nMol..... 4507nMol.. 2.00%........ 4518nMol. 4.00%......... 6.00%......... 8.00%....... 4547nMol.. 10.00%......... 12.00%.. 4562nMol....... 14.00%......... 16.00%......... 18.00%......... 20.00%......... 22.00%......... 24.00%......... 26.00%......... 28.00%......... 30.00%......... 32.00%........ 4668nMol. 34.00%......... 4679nMol 36.00%......... 4689nMol 38.00% 4690nMol. 4691nMol........ 40.00%..... 4705nMol.... 42.00%....... 4717nMol.. 44.00%....... 4727nMol.. 46.00%. 4731nMol..... 4736nMol. 4737nMol. 4738nMol. 48.00%... 4743nMol. 4744nMol..... 50.00%.. 4752nMol.. 4754nMol. 4755nMol. 4756nMol. 4757nMol.. 52.00%......... 54.00% 4770nMol....... 4777nMol. 4778nMol. 56.00%.. 4782nMol. 4783nMol...... 58.00%... 4793nMol... 4796nMol... 60.00%. 4801nMol... 4804nMol. 4805nMol. 4806nMol. 4807nMol.. 62.00%. 4811nMol. 4812nMol..... 4817nLink. 4818nLink. 4819nLink 64.00% 4820nLink. 4821nLink. 4822nLink. 4823nLink. 4824nLink. 4825nLink. 4826nLink. 4827nLink. 4828nLink. 4829nLink 66.00% 4830nLink. 4831nLink. 4832nLink. 4833nLink. 4834nLink. 4835nLink. 4836nLink. 4837nLink. 4838nLink. 4839nLink 68.00% 4840nLink. 4841nLink. 4842nLink. 4843nLink. 4844nLink. 4845nLink. 4846nLink. 4847nLink. 4848nLink. 4849nLink 70.00% 4850nLink. 4851nLink. 4852nLink. 4853nLink. 4854nLink. 4855nLink. 4856nLink. 4857nLink. 4858nLink. 4859nLink 72.00% 4860nLink. 4861nLink. 4862nLink. 4863nLink. 4864nLink. 4865nLink. 4866nLink. 4867nLink. 4868nLink. 4869nLink 74.00% 4870nLink. 4871nLink. 4872nLink. 4873nLink. 4874nLink. 4875nLink. 4876nLink. 4877nLink. 4878nLink. 4879nLink 76.00% 4880nLink. 4881nLink. 4882nLink. 4883nLink. 4884nLink. 4885nLink. 4886nLink. 4887nLink. 4888nLink. 4889nLink 78.00% 4890nLink. 4891nLink. 4892nLink. 4893nLink. 4894nLink. 4895nLink. 4896nLink. 4897nLink. 4898nLink. 4899nLink 80.00% 4900nLink. 4901nLink. 4902nLink. 4903nLink. 4904nLink. 4905nLink. 4906nLink. 4907nLink. 4908nLink. 4909nLink 82.00% 4910nLink. 4911nLink. 4912nLink. 4913nLink. 4914nLink. 4915nLink. 4916nLink. 4917nLink. 4918nLink. 4919nLink 84.00% 4920nLink. 4921nLink. 4922nLink. 4923nLink. 4924nLink. 4925nLink. 4926nLink. 4927nLink. 4928nLink. 4929nLink 86.00% 4930nLink. 4931nLink. 4932nLink. 4933nLink. 4934nLink. 4935nLink. 4936nLink. 4937nLink. 4938nLink. 4939nLink 88.00% 4940nLink. 4941nLink. 4942nLink. 4943nLink. 4944nLink. 4945nLink. 4946nLink. 4947nLink. 4948nLink. 4949nLink 90.00% 4950nLink. 4951nLink. 4952nLink. 4953nLink. 4954nLink. 4955nLink. 4956nLink. 4957nLink. 4958nLink. 4959nLink 92.00% 4960nLink. 4961nLink. 4962nLink. 4963nLink. 4964nLink. 4965nLink. 4966nLink. 4967nLink. 4968nLink. 4969nLink 94.00% 4970nLink. 4971nLink. 4972nLink. 4973nLink. 4974nLink. 4975nLink. 4976nLink. 4977nLink. 4978nLink. 4979nLink 96.00% 4980nLink. 4981nLink. 4982nLink. 4983nLink. 4984nLink. 4985nLink. 4986nLink. 4987nLink. 4988nLink. 4989nLink 98.00% 4990nLink. 4991nLink. 4992nLink. 4993nLink. 4994nLink. 4995nLink. 4996nLink. 4997nLink. 4998nLink. 4999nLink 100.00%"
     ]
    }
   ],
   "source": [
    "dicty4500to5000 = dictMaker(range(4500,5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('totalDict.pickle', 'wb') as f:\n",
    "    pickle.dump(totalDict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2763"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def getDictLists(sourceKey, dictionary=femaDict):\n",
    "    '''\n",
    "    Generator returns key, and result from a dictionary's given sourceKey when available.\n",
    "    If the result is not available returns NoneType\n",
    "    '''\n",
    "    for key in dictionary.keys():\n",
    "        try:\n",
    "            ans = dictionary[key].get(sourceKey)\n",
    "        except:\n",
    "            continue\n",
    "        if ans:\n",
    "            yield key, ans\n",
    "\n",
    "def phraseToSubWord(phraseList, function1, function2):\n",
    "    '''\n",
    "    processes a phrase to a list of words treated by functions 1 and 2\n",
    "    '''\n",
    "    newList = []\n",
    "    for phrase in phraseList:\n",
    "        newPhrase = []\n",
    "        words = phrase.split()\n",
    "        for word in words:\n",
    "            newWord = function1(word)\n",
    "            newWord = function2(newWord)\n",
    "            newPhrase.append(newWord)\n",
    "        ', '.join(newPhrase)\n",
    "        newList.extend(newPhrase)\n",
    "    return newList\n",
    "\n",
    "def dictKeyConverter(function1, function2, sourceKey, newKey, dictionary=femaDict):\n",
    "    '''\n",
    "    Applies phraseToSubWord() to sourceKey in femDict and adds the results to newKey\n",
    "    '''\n",
    "    \n",
    "    stemDescriptors = []\n",
    "    for key, result in getDictLists(sourceKey):\n",
    "        newList = phraseToSubWord(result, \n",
    "                                  function1, \n",
    "                                  function2)\n",
    "        if key == 'single descriptor counts':\n",
    "            print(key, result,newList)\n",
    "        dictionary[key][newKey] = newList\n",
    "    return dictionary\n",
    "\n",
    "def dictSingles(function1, function2, sourceKey, listKey, indexKey, countKey, dictionary=femaDict):\n",
    "    '''\n",
    "    returns input dictionary with a:\n",
    "    -listKey with a set-list of alphabetized descriptors used in a dictionary sourceKey list \n",
    "    as single words, not part of a phrase\n",
    "    -indexKey with a dictionary using the words in the listKey list as keys and an associated index\n",
    "    -countKey with a list of tuples with the count numbers of the single descriptors in the dictionary\n",
    "    '''\n",
    "\n",
    "    #create single descriptor list from the sourceKey entrys in dictionary\n",
    "    singleDescriptors = []\n",
    "    for _, result in getDictLists(sourceKey): \n",
    "        for phrase in result:\n",
    "            if 0 < len(phrase.split()) < 3:\n",
    "                singleDescriptors.append(phrase)\n",
    "    \n",
    "    #process singleDescriptors with two functions, count the occurences and assign count to countKey \n",
    "    singleDescriptors = phraseToSubWord(singleDescriptors,\n",
    "                                        function1,\n",
    "                                        function2)\n",
    "    singleCounter = Counter(singleDescriptors).most_common()\n",
    "    dictionary[countKey] = singleCounter\n",
    "    \n",
    "    #narrow down to a set, sort, and assign singleDescriptor list to listKey\n",
    "    singleList= list(set(singleDescriptors))\n",
    "    singleList.sort()\n",
    "    dictionary[listKey] = singleList\n",
    "    \n",
    "    #create index dictionary of singleList elements for indexKey\n",
    "    descriptorIndices = {}\n",
    "    for i, word in enumerate(singleList):\n",
    "        descriptorIndices[word] = i\n",
    "    dictionary[indexKey] = descriptorIndices\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CAS': '26446-38-8', 'rejected descriptors': ['waxi', 'also', 'help', 'to', 'emulsifi', 'other', 'compon', 'within', 'the', 'flavor', 'formul'], 'name': 'SUCROSE MONOPALMITATE', 'descriptor indices': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'stemmed descriptors': ['fatti', 'waxi', 'can', 'also', 'help', 'to', 'emulsifi', 'other', 'compon', 'within', 'the', 'flavor', 'formul'], 'descriptors': ['fatty', 'waxy; can also help to emulsify other components within the flavor formulation'], 'link': 'http://www.femaflavor.org/flavor/library/sucrose-monopalmitate', 'selected descriptors': ['fatti', 'can'], 'FEMA': '4713', 'rdkit Mol': <rdkit.Chem.rdchem.Mol object at 0x113fbc1f0>}\n"
     ]
    }
   ],
   "source": [
    "from nltk import stem\n",
    "import re\n",
    "\n",
    "pattern = re.compile('[\\W_]+')\n",
    "function1 = lambda x: pattern.sub('', x)\n",
    "stemmer = stem.SnowballStemmer('english')\n",
    "function2 = stemmer.stem\n",
    "\n",
    "femaDict = dictKeyConverter(function1, function2, \n",
    "                            sourceKey='descriptors', \n",
    "                            newKey='stemmed descriptors')\n",
    "\n",
    "femaDict = dictSingles(function1, function2,\n",
    "                       sourceKey='descriptors',\n",
    "                       listKey='single descriptors', \n",
    "                       indexKey='descriptor index pairs', \n",
    "                       countKey='single descriptor counts')\n",
    "\n",
    "print(femaDict['4713'])\n",
    "# print(femaDict['single descriptor counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def descriptorSelector(listKey, sourceKey, selectKey, rejectKey, dictionary=femaDict):\n",
    "    '''\n",
    "    returns an input dictionary with two new keys based on the input dictionary's sourceKey list:\n",
    "    selectKey has a list containing items found in singleList and the sourceKey list\n",
    "    rejectKey has a list containing items not found in singleList but in the sourceKey list\n",
    "    '''\n",
    "    \n",
    "    for key, result in getDictLists(sourceKey):\n",
    "        selectList = []\n",
    "        rejects = []\n",
    "        for word in result:\n",
    "            #print('word: {} ' .format(word))\n",
    "            if word in dictionary[listKey]:\n",
    "                selectList.append(word)\n",
    "                #print('selectList: {} ' .format(selectList))\n",
    "            else:\n",
    "                rejects.append(word)\n",
    "                #print('rejects: {} ' .format(rejects))\n",
    "                #print('counter: {} ' .format(counter))\n",
    "        if len(selectList) > 0:\n",
    "            #print('length selectList: {} ' .format(len(selectList)))\n",
    "            dictionary[key][selectKey] = selectList\n",
    "        if len(rejects) > 0:\n",
    "            #print('length rejects: {} ' .format(len(rejects)))\n",
    "            dictionary[key][rejectKey] = rejects\n",
    "    return dictionary\n",
    "\n",
    "def selectIndicesMaker(listKey, sourceKey, indexKey,  newKey, dictionary=femaDict):\n",
    "    '''\n",
    "    returns an input dictionary with new keys based on the input dictionary's sourceKey list:\n",
    "    selectKey has a list containing items found in singleList and the sourceKey list\n",
    "    rejectKey has a list containing items not found in singleList but in the sourceKey list\n",
    "    '''\n",
    "    zeros = [0] * len(dictionary[listKey])\n",
    "    for key, result in getDictLists(sourceKey):\n",
    "        newList = zeros[:]\n",
    "        for word in result:\n",
    "            idx = dictionary[indexKey].get(word)\n",
    "            if idx != None: #can't use simple if because it will discount 0 index\n",
    "                newList[idx] = 1\n",
    "            else:\n",
    "                print(key, word)\n",
    "        dictionary[key][newKey] = newList\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "femaDict = descriptorSelector(listKey ='single descriptors', \n",
    "                              sourceKey='stemmed descriptors', \n",
    "                              selectKey='selected descriptors', \n",
    "                              rejectKey='rejected descriptors')\n",
    "\n",
    "femaDict = selectIndicesMaker(listKey='single descriptors',\n",
    "                              sourceKey='selected descriptors',\n",
    "                              indexKey='descriptor index pairs', \n",
    "                              newKey='descriptor indices')\n",
    "print(femaDict['4713'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def makeDataFrame(listKey, indexKey, dictionary=femaDict):\n",
    "    '''\n",
    "    Returns a pandas DataFrame from lists found in dictionary.\n",
    "    Uses listKey as the column index in DataFrame. \n",
    "    As such listKey list must have the same length as the indexKey list. \n",
    "    Relies  on makeArrayRow function.\n",
    "    '''  \n",
    "    array = [[0]*(len(dictionary[listKey])+1)]#empty row to initialize array. will get ignored downstream\n",
    "    for key, result in getDictLists(indexKey):\n",
    "        newRow = [[int(key)] + result]\n",
    "        array = np.concatenate((array, newRow), axis=0)\n",
    "    data = array[1:,1:] #takes only real data, ignores first filler row and 1st index column\n",
    "    index = array[1:,0] #uses the first column as index\n",
    "    df = pd.DataFrame(data, index=index, columns=dictionary[listKey])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = makeDataFrame('single descriptors', 'descriptor indices')\n",
    "\n",
    "#compares each row in the dataFrame to its counterpart in the dictionary\n",
    "for key, result in getDictLists('descriptor indices'):\n",
    "    if (list(df.loc[int(key)]) != result): \n",
    "        print(key)\n",
    "        break\n",
    "print('All rows in DataArray match!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def makeWordCould(text, title):\n",
    "    wordcloud = WordCloud().generate(text)\n",
    "    plt.figure(figsize = (14,7))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title, size=30)\n",
    "    plt.show()\n",
    "\n",
    "def listToText(wordList):\n",
    "    '''\n",
    "    input: list of strings (words)\n",
    "    returns: a string made out of the strings in the list separated by spaces\n",
    "    '''\n",
    "    text = ''\n",
    "    for word in wordList:\n",
    "        text = text + ' ' + word\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = femaDict['single descriptor counts']\n",
    "text = ''\n",
    "for item in count:\n",
    "    for _ in range(item[1]):\n",
    "        text = text + ' ' + item[0]\n",
    "\n",
    "makeWordCould(text, 'Stemmed descriptors used for selection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meatList = list(df[df.meat == 1].index)\n",
    "tallowList = list(df[df.tallow ==1].index)\n",
    "beefList = list(df[df.beef ==1].index)\n",
    "meatIndices = list(set(meatList+tallowList+beefList))\n",
    "len(meatIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "meatDescriptors = []\n",
    "for key in meatIndices:\n",
    "    tempList = femaDict[str(key)]['stemmed descriptors']\n",
    "    meatDescriptors.extend(tempList)\n",
    "text = ''\n",
    "for word in meatDescriptors:\n",
    "    text = text + ' ' + word\n",
    "\n",
    "makeWordCould(text, 'Stemmed descriptors in meaty chemicals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meatDescriptors = []\n",
    "for key in meatIndices:\n",
    "    tempList = femaDict[str(key)]['selected descriptors']\n",
    "    meatDescriptors.extend(tempList)\n",
    "text = ''\n",
    "for word in meatDescriptors:\n",
    "    text = text + ' ' + word\n",
    "\n",
    "makeWordCould(text, 'Selected descriptors in meaty chemicals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "X = df.values\n",
    "totalLabels = len(df.columns)\n",
    "nums = []\n",
    "unities = []\n",
    "for num in range(int(totalLabels/2), 0, -1):\n",
    "    kmeans = KMeans(n_clusters=num, random_state=0).fit(X)\n",
    "    dfTest = pd.DataFrame(kmeans.labels_, index = df.index)\n",
    "    labels = dfTest.loc[meatIndices].values.tolist()\n",
    "    labels = [label for lst in labels for label in lst]\n",
    "    unity = len(set(labels))\n",
    "    nums.append(num)\n",
    "    unities.append(unity)\n",
    "    if num%5 == 0:\n",
    "        print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ratios = np.array(unities)/np.array(nums)\n",
    "\n",
    "plt.figure(1)\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.plot(nums, ratios)\n",
    "plt.axis([0, 40, 0, 1])\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(nums, unities)\n",
    "plt.axis([0, 40, 0, 11])\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.plot(nums, ratios)\n",
    "plt.axis([22, 27, 0.2, 0.5])\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.plot(nums, unities)\n",
    "plt.axis([22, 27, 5, 10])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df.values\n",
    "kmeans = KMeans(n_clusters=25, random_state=0).fit(X)\n",
    "dfTest = pd.DataFrame(kmeans.labels_, index = df.index)\n",
    "labels = dfTest.loc[meatIndices].values.tolist()\n",
    "labels = [label for lst in labels for label in lst]\n",
    "set(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To create word clouds for each of the labels generated at 25 total labels\n",
    "X = df.values\n",
    "kmeans = KMeans(n_clusters=25, random_state=0).fit(X)\n",
    "dfLabels = pd.DataFrame(kmeans.labels_, index = df.index, columns=['labels'])\n",
    "\n",
    "# Determine the labels associated with Meat (meatIndices)\n",
    "labels = dfTest.loc[meatIndices].values #.tolist()\n",
    "labels = [label for lst in labels for label in lst]\n",
    "labelCount = Counter(labels).most_common()\n",
    "labels = set(labels)\n",
    "\n",
    "for label in labels:\n",
    "    descriptors = []\n",
    "    #find the associated fema numbers\n",
    "    femaNums = dfLabels[dfLabels['labels'] == label].index.tolist()\n",
    "    #for each fema number:\n",
    "    for num in femaNums:\n",
    "        #extract the select descriptors\n",
    "        tempList = femaDict[str(num)]['selected descriptors']\n",
    "        #compile a list of the descriptors\n",
    "        descriptors.extend(tempList)\n",
    "    #make a wordCloud\n",
    "    meatCount = [v[1] for v in labelCount if v[0]==label]\n",
    "    title = 'Label: ' + str(label) + '; Number of chemicals: ' + str(meatCount[0]) + '/' +  str(len(femaNums)) \n",
    "    makeWordCould(listToText(descriptors), title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findEmptyKeys(dictionary, descriptorKey):\n",
    "    notEmpty = []\n",
    "    empty = []\n",
    "    \n",
    "    for key in dictionary.keys():\n",
    "        test = dictionary[key].get(descriptorKey)\n",
    "        if test:\n",
    "            notEmpty.append(key)\n",
    "        else:\n",
    "            empty.append(key)\n",
    "    \n",
    "    print('Not empty:\\n')\n",
    "    for item in notEmpty[0:5]:\n",
    "        print(item)\n",
    "    print('\\nEmpty:')\n",
    "    for item in empty[0:5]:\n",
    "        print(item)\n",
    "    \n",
    "    return\n",
    "\n",
    "findEmptyKeys(femaDict, 'select descriptor indices')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
